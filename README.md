# **BusinessLLMRAGChat**

A full-stack, production-grade LLM application that fine-tunes open-source conversational models and deploys them for efficient Retrieval-Augmented Generation (RAG) with LangGraph and vLLM â€” packaged with Streamlit Cloud UI for business demo.

---

## ğŸš€ Project Overview

**BusinessLLMRAGChat** is an end-to-end project combining **LLM fine-tuning**, **evaluation**, **accelerated inference**, and **RAG chat demo** to showcase how fine-tuned models can be integrated with external knowledge for business rule reasoning and QA.

This project simulates a real-world pipeline for fine-tuning open-source LLMs (LoRA/SFT), evaluating performance (BLEU, cosine, judge), comparing multiple runs (MLflow), and deploying the best-performing model using `FastAPI + vLLM` for inference.

It ends with a LangGraph-powered **Streamlit Chat App** that supports business-rule-aware question answering and semantic search (via FAISS).

---

## ğŸ§  Architecture

```
+----------------+       +----------------+       +------------------+       +------------------+
|                |       |                |       |                  |       |                  |
|   User Input   +------>+  Streamlit UI  +------>+     FastAPI      +------>+      vLLM        |
|                |       |  (LangGraph)   |       |   (EC2 Inference) |       | (Accelerated LLM)|
+----------------+       +----------------+       +------------------+       +------------------+
                                                   |
                          +------------------------+-------------------------+
                          |
                          |     Retrieval
                          v
                  [FAISS Vector DB on business rule corpus]
```

---

## ğŸ“¦ Techniques Used

### ğŸ“Š **1. Dataset & Preprocessing**
- OpenAssistant Conversations Dataset (OASST1)
- Converted to `{"text": "Instruction: ...\nResponse: ..."}` format
- Stored in S3 (`train.jsonl`, `eval.jsonl`)

### ğŸ”§ **2. Fine-Tuning Techniques**
- LoRA (Low-Rank Adaptation)
- SFT (Supervised Fine-Tuning)
- Unsloth: Speed-optimized PEFT trainer
- Hugging Face `transformers` + `peft` + `datasets`

### ğŸ“ˆ **3. Experiment Tracking**
- MLflow: Monitor runs, compare BLEU, cosine similarity
- Metrics logging + artifact versioning
- Multiple runs: compare LoRA vs. SFT

### ğŸï¸ **4. Inference Engine**
- **vLLM**: Token streaming + KV cache optimization
- FastAPI for RESTful inference interface
- Dockerized microservices (FastAPI + vLLM split or combined on EC2 G5)

### ğŸ” **5. RAG (Retrieval-Augmented Generation)**
- FAISS similarity search over business rule documents
- LangGraph handles prompt routing and similarity response
- Streamlit connects to FastAPI and triggers semantic retrieval

### ğŸ¯ **6. Deployment**
- Streamlit Cloud frontend demo (LangGraph + chat history)
- EC2 (G5) for both inference containers
- Docker + Poetry for environment reproducibility

---

## ğŸ—‚ Directory Structure

```bash
BusinessLLMRAGChat/
â”œâ”€â”€ pyproject.toml                 # Poetry config
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ convert_oasst.py          # Download and convert dataset
â”œâ”€â”€ src/businessllmragchat/
â”‚   â”œâ”€â”€ fine_tune/                # train.py, evaluate.py
â”‚   â”œâ”€â”€ rag_langgraph/            # LangGraph RAG pipeline
â”‚   â””â”€â”€ inference_api/            # serve_vllm.py, fastapi_app.py
â”œâ”€â”€ inference_api/
â”‚   â”œâ”€â”€ Dockerfile.vllm
â”‚   â”œâ”€â”€ Dockerfile.fastapi
â”‚   â”œâ”€â”€ requirements_vllm.txt
â”‚   â””â”€â”€ requirements_api.txt
â”œâ”€â”€ streamlit_demo/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ tests/
    â”œâ”€â”€ test_train.py
    â””â”€â”€ test_inference.py
```

---

## ğŸ› ï¸ Requirements

- Python 3.10+
- Poetry
- EC2 G5.2xlarge (24GB VRAM) or higher
- Docker
- Hugging Face account (for model downloading)
- AWS IAM role with:
  - `AmazonS3FullAccess`
  - `bedrock:InvokeModel` (optional, if using Bedrock baseline)

---

## ğŸ§ª Evaluation Metrics

| Metric    | Description                                      |
|-----------|--------------------------------------------------|
| **BLEU**  | N-gram overlap between predicted & reference     |
| **Cosine**| Embedding similarity (sentence-transformers)     |
| **Judge** | LLM-based preference evaluation (optional)       |

---

## ğŸ“¦ Deployment Plan

1. ğŸ“ Train models using `train.py`
2. ğŸ§ª Evaluate & log runs with MLflow
3. ğŸ¥‡ Pick best run â†’ serve with FastAPI or vLLM
4. ğŸš€ Deploy UI via Streamlit Cloud
5. ğŸ” LangGraph orchestrates retrieval + inference

---

## ğŸ’¼ Why This Project Matters

This project simulates a real-world **AI/ML Engineer** workflow:
- Model fine-tuning
- Multi-run comparison & selection
- Inference optimization (vLLM)
- Integration with LangGraph RAG
- Full-stack deployment (backend + frontend)

Ideal as a **portfolio project** to showcase:
- LLM customization
- Infra deployment (EC2, Docker)
- RAG integration for business use cases
- DevOps-aware ML practices (MLflow + containerization)
